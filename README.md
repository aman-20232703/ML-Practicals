# Complete Machine Learning Guide for Beginners

## Visualization Companion Script

To explore the outputs of every ML practical interactively, run `python ML\visualize_ml_topics.py` from this repository. Close each figure window to advance to the next plot.

- `python ML\visualize_ml_topics.py --list` shows each section and subtopic.
- `python ML\visualize_ml_topics.py --section data_preprocessing` runs the visualizations for that section only; available sections are `data_preprocessing`, `linear_regression`, `supervised_learning`, and `unsupervised_learning`.
- `python ML\visualize_ml_topics.py --topic preprocessing_standardization_manual` renders an individual chart; use `--list` to look up valid topic keys.

The script mirrors the structure of the `ML` folder and includes dedicated graphs for normalization, standardization, linear regression (manual and sklearn variants), supervised learning metrics and projections, as well as clustering and dimensionality reduction techniques.

### Visualization Coverage

- **Data preprocessing**: side-by-side MinMaxScaler bars, manual vs sklearn normalization comparisons, StandardScaler z-score traces, and manual z-score error bars to highlight scale differences.
- **Linear regression**: scatter plots with regression lines for sklearn and manual fits, annotated predictions for new points, and residual stem plots to illustrate model error.
- **Supervised learning**: accuracy bar charts, confusion-matrix grids for each classifier, PCA scatter projections of the breast-cancer dataset, RÂ² vs actual/predicted overlays, and sequential prediction traces across regression models.
- **Unsupervised learning**: k-means, agglomerative, and DBSCAN cluster scatter plots on synthetic blobs plus PCA and t-SNE embeddings for high-dimensional samples.

## Unit 1: Introduction to Machine Learning

### What is Machine Learning?

Machine Learning is a branch of artificial intelligence where computers learn from data without being explicitly programmed. Instead of writing specific rules, we feed data to algorithms that learn patterns and make predictions.

**Real-life analogy**: Think of teaching a child to recognize animals. Instead of explaining "a cat has pointy ears, whiskers, and says meow," you show them many pictures of cats. Eventually, they learn to identify cats on their own.

### Key Elements of Machine Learning

#### 1. **Data**

The fuel for ML. Without data, there's no learning.

- **Example**: Customer purchase history, weather records, medical images

#### 2. **Features**

Characteristics or attributes of your data.

- **Example**: For house price prediction - number of bedrooms, location, square footage

#### 3. **Model**

The algorithm that learns patterns from data.

#### 4. **Training**

The process of feeding data to the model so it can learn.

#### 5. **Prediction**

Using the trained model on new, unseen data.

### Types of Machine Learning

#### 1. **Supervised Learning**

Learning with labeled data (you know the correct answers).

**Real-life Example**: Email spam detection

- You have emails labeled as "spam" or "not spam"
- The model learns patterns from these labeled emails
- It can then classify new emails

```python
# Simple Supervised Learning Example - Email Classification
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# Sample data: emails and their labels
emails = [
    "Win free money now!!!",
    "Meeting scheduled for tomorrow",
    "Congratulations! You won a prize!",
    "Project deadline reminder",
    "Click here for amazing offers",
    "Team lunch at 1 PM"
]

labels = ["spam", "not spam", "spam", "not spam", "spam", "not spam"]

# Convert text to numbers (vectorization)
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(emails)

# Split data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)

# Train the model
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict new email
new_email = ["Free prize waiting for you"]
new_email_vectorized = vectorizer.transform(new_email)
prediction = model.predict(new_email_vectorized)
print(f"Email classification: {prediction[0]}")
```

#### 2. **Unsupervised Learning**

Learning from unlabeled data (finding hidden patterns).

**Real-life Example**: Customer segmentation

- A store wants to group customers by buying behavior
- No predefined categories exist
- The algorithm discovers natural groupings

```python
# Unsupervised Learning Example - Customer Segmentation
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt

# Customer data: [Annual Income (k$), Spending Score (1-100)]
customers = np.array([
    [15, 39], [15, 81], [16, 6], [16, 77],
    [17, 40], [17, 76], [18, 6], [18, 94],
    [19, 3], [19, 72], [20, 14], [20, 99],
    [70, 20], [72, 35], [73, 43], [74, 50],
    [75, 42], [76, 10], [77, 48], [78, 85]
])

# Apply K-Means clustering (find 3 groups)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(customers)

print(f"Customer segments: {clusters}")
print(f"Cluster centers: {kmeans.cluster_centers_}")
```

#### 3. **Reinforcement Learning** (Introduction)

Learning through trial and error with rewards and penalties.

**Real-life Example**: Teaching a robot to walk

- Robot tries different movements
- Gets rewards for successful steps
- Gets penalties for falling
- Learns optimal walking strategy over time

**Other Examples**:

- Game AI (Chess, Go)
- Self-driving cars
- Recommendation systems that improve over time

---

## Unit 2: Preprocessing

Data preprocessing is like preparing ingredients before cooking. Raw data is often messy and needs cleaning.

### Feature Scaling

**What is it?** Making all features have similar ranges so one feature doesn't dominate others.

**Real-life Example**: Imagine predicting house prices using:

- Square footage: 500-5000
- Number of bedrooms: 1-5

Without scaling, square footage would dominate because its values are much larger.

#### Types of Feature Scaling:

**1. Normalization (Min-Max Scaling)**
Scales data to range [0, 1]

```python
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# House data: [square_feet, bedrooms, age_years]
houses = np.array([
    [1500, 3, 10],
    [2500, 4, 5],
    [800, 2, 15],
    [3200, 5, 2]
])

scaler = MinMaxScaler()
houses_normalized = scaler.fit_transform(houses)

print("Original data:")
print(houses)
print("\nNormalized data (0-1 range):")
print(houses_normalized)
```

**2. Standardization (Z-score Normalization)**
Centers data around mean=0, standard deviation=1

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
houses_standardized = scaler.fit_transform(houses)

print("Standardized data:")
print(houses_standardized)
```

### Feature Selection Methods

**Why?** Not all features are useful. Some might be redundant or irrelevant.

**Real-life Example**: Predicting student exam scores

- Useful features: study hours, attendance, previous grades
- Useless features: shoe size, favorite color

#### Methods:

**1. Filter Methods**
Select features based on statistical scores.

```python
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.datasets import load_iris

# Load sample data
iris = load_iris()
X, y = iris.data, iris.target

# Select top 2 features
selector = SelectKBest(chi2, k=2)
X_new = selector.fit_transform(X, y)

print(f"Original features: {X.shape[1]}")
print(f"Selected features: {X_new.shape[1]}")
print(f"Selected feature indices: {selector.get_support(indices=True)}")
```

**2. Wrapper Methods**
Use model performance to select features.

**3. Embedded Methods**
Feature selection happens during model training.

### Dimensionality Reduction - Principal Component Analysis (PCA)

**What is it?** Reducing the number of features while keeping most information.

**Real-life Example**: Imagine describing a person

- Instead of 100 characteristics (height, weight, age, hair color, eye color...)
- You use 3 main "components" that capture most information (physical build, age group, appearance)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

# Load handwritten digits (64 features per digit)
digits = load_digits()
X, y = digits.data, digits.target

print(f"Original features: {X.shape[1]}")

# Reduce to 2 dimensions for visualization
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

print(f"Reduced features: {X_reduced.shape[1]}")
print(f"Variance explained: {pca.explained_variance_ratio_}")

# Visualize
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='tab10', alpha=0.5)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA: 64 dimensions reduced to 2')
plt.colorbar(scatter)
plt.show()
```

---

## Unit 3: Regression

**What is Regression?** Predicting continuous numerical values.

**Real-life Examples**:

- Predicting house prices
- Forecasting stock prices
- Estimating temperature
- Predicting salary based on experience

### Linear Regression with One Variable

**Concept**: Finding a straight line that best fits the data.

**Formula**: y = mx + b

- y = predicted value
- m = slope (weight)
- x = input feature
- b = intercept (bias)

**Real-life Example**: Predicting house price based on size

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# House size (sq ft) and prices ($1000s)
house_size = np.array([800, 1000, 1200, 1400, 1600, 1800, 2000, 2200]).reshape(-1, 1)
house_price = np.array([150, 180, 200, 230, 250, 280, 300, 330])

# Create and train model
model = LinearRegression()
model.fit(house_size, house_price)

# Make predictions
predictions = model.predict(house_size)

print(f"Slope (m): {model.coef_[0]:.2f}")
print(f"Intercept (b): {model.intercept_:.2f}")
print(f"Equation: Price = {model.coef_[0]:.2f} * Size + {model.intercept_:.2f}")

# Predict a new house
new_house_size = np.array([[1500]])
predicted_price = model.predict(new_house_size)
print(f"\nPredicted price for 1500 sq ft: ${predicted_price[0]:.2f}k")

# Visualize
plt.scatter(house_size, house_price, color='blue', label='Actual')
plt.plot(house_size, predictions, color='red', label='Predicted')
plt.xlabel('House Size (sq ft)')
plt.ylabel('Price ($1000s)')
plt.legend()
plt.title('Linear Regression: House Price Prediction')
plt.show()
```

### Linear Regression with Multiple Variables

**Concept**: Using multiple features to make predictions.

**Real-life Example**: Predicting house price based on size, bedrooms, and age

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Multiple features: [size, bedrooms, age]
X = np.array([
    [1500, 3, 10],
    [2500, 4, 5],
    [800, 2, 15],
    [3200, 5, 2],
    [1800, 3, 8],
    [2200, 4, 6],
    [1000, 2, 12],
    [2800, 4, 3]
])

# Prices
y = np.array([250, 400, 180, 480, 290, 350, 200, 420])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)

print("Feature coefficients:")
print(f"Size: {model.coef_[0]:.4f}")
print(f"Bedrooms: {model.coef_[1]:.4f}")
print(f"Age: {model.coef_[2]:.4f}")
print(f"\nIntercept: {model.intercept_:.2f}")

# Evaluate
mse = mean_squared_error(y_test, predictions)
r2 = r2_score(y_test, predictions)
print(f"\nMean Squared Error: {mse:.2f}")
print(f"RÂ² Score: {r2:.4f}")
```

### Gradient Descent

**What is it?** An optimization algorithm to find the best parameters (weights) for your model.

**Real-life Analogy**: Imagine you're hiking down a mountain in thick fog. You can only see your immediate surroundings. You take small steps in the direction that goes down the most steeply. Eventually, you reach the bottom (minimum error).

**How it works**:

1. Start with random weights
2. Calculate error
3. Adjust weights to reduce error
4. Repeat until error is minimized

```python
# Gradient Descent from scratch
def gradient_descent_demo():
    # Simple dataset
    X = np.array([1, 2, 3, 4, 5])
    y = np.array([2, 4, 6, 8, 10])  # y = 2*x (perfect linear relationship)

    # Initialize parameters
    m = 0  # slope
    b = 0  # intercept
    learning_rate = 0.01
    iterations = 1000
    n = len(X)

    # Gradient Descent
    for i in range(iterations):
        # Predictions
        y_pred = m * X + b

        # Calculate gradients
        dm = (-2/n) * sum(X * (y - y_pred))
        db = (-2/n) * sum(y - y_pred)

        # Update parameters
        m = m - learning_rate * dm
        b = b - learning_rate * db

        if i % 100 == 0:
            mse = sum((y - y_pred)**2) / n
            print(f"Iteration {i}: m={m:.4f}, b={b:.4f}, MSE={mse:.4f}")

    print(f"\nFinal equation: y = {m:.4f}x + {b:.4f}")

gradient_descent_demo()
```

### Overfitting and Regularization

**Overfitting**: Model learns training data too well (memorizes noise).

**Real-life Analogy**: A student who memorizes answers instead of understanding concepts. They do great on practice tests but fail on new questions.

**Regularization**: Techniques to prevent overfitting.

#### Types of Regularization:

**1. Ridge Regression (L2 Regularization)**
Adds penalty for large coefficients.

```python
from sklearn.linear_model import Ridge

# Create polynomial features that might overfit
from sklearn.preprocessing import PolynomialFeatures

X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([2, 4, 6, 8, 10, 12])

# Create polynomial features
poly = PolynomialFeatures(degree=5)
X_poly = poly.fit_transform(X)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)  # alpha controls regularization strength
ridge_model.fit(X_poly, y)

print("Ridge coefficients:", ridge_model.coef_)
```

**2. Lasso Regression (L1 Regularization)**
Can reduce coefficients to zero (feature selection).

```python
from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.1)
lasso_model.fit(X_poly, y)

print("Lasso coefficients:", lasso_model.coef_)
```

### Regression Evaluation Metrics

**1. Mean Squared Error (MSE)**
Average of squared differences between predicted and actual values.

```python
from sklearn.metrics import mean_squared_error

y_true = [3, -0.5, 2, 7]
y_pred = [2.5, 0.0, 2, 8]
mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse}")
```

**2. RÂ² Score (Coefficient of Determination)**
How well predictions match actual values (0-1, higher is better).

```python
from sklearn.metrics import r2_score

r2 = r2_score(y_true, y_pred)
print(f"RÂ² Score: {r2}")
```

---

## Unit 4: Classification

**What is Classification?** Predicting discrete categories or classes.

**Real-life Examples**:

- Email: Spam or Not Spam
- Medical: Disease or Healthy
- Bank: Approve or Reject Loan
- Image: Cat, Dog, or Bird

### Decision Trees

**Concept**: Makes decisions by asking yes/no questions, like a flowchart.

**Real-life Example**: Deciding whether to play tennis based on weather

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load iris dataset (flower classification)
iris = load_iris()
X, y = iris.data, iris.target

# Train Decision Tree
tree_model = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_model.fit(X, y)

# Visualize the tree
plt.figure(figsize=(20, 10))
plot_tree(tree_model, feature_names=iris.feature_names,
          class_names=iris.target_names, filled=True, fontsize=10)
plt.title('Decision Tree for Iris Classification')
plt.show()

# Make prediction
sample = [[5.1, 3.5, 1.4, 0.2]]  # Sample flower measurements
prediction = tree_model.predict(sample)
print(f"Predicted class: {iris.target_names[prediction[0]]}")
```

### Naive Bayes Classifier

**Concept**: Uses probability based on Bayes' Theorem. Assumes features are independent.

**Real-life Example**: Spam email detection based on word frequency

```python
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

# Generate sample dataset
X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Naive Bayes
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Predict and evaluate
accuracy = nb_model.score(X_test, y_test)
print(f"Naive Bayes Accuracy: {accuracy:.4f}")

# Predict probabilities
sample = [[0.5, 1.2, -0.3, 0.8]]
probabilities = nb_model.predict_proba(sample)
print(f"Prediction probabilities: {probabilities}")
```

### Logistic Regression

**Concept**: Despite its name, it's used for classification. Predicts probability of belonging to a class.

**Real-life Example**: Predicting if a student will pass or fail based on study hours

```python
from sklearn.linear_model import LogisticRegression

# Study hours and pass/fail (1=pass, 0=fail)
study_hours = np.array([[1], [2], [3], [4], [5], [6], [7], [8]]).reshape(-1, 1)
passed = np.array([0, 0, 0, 1, 1, 1, 1, 1])

# Train logistic regression
log_model = LogisticRegression()
log_model.fit(study_hours, passed)

# Predict for a new student
new_student_hours = np.array([[4.5]])
probability = log_model.predict_proba(new_student_hours)
prediction = log_model.predict(new_student_hours)

print(f"Probability of passing: {probability[0][1]:.4f}")
print(f"Prediction: {'Pass' if prediction[0] == 1 else 'Fail'}")
```

### K-Nearest Neighbors (KNN)

**Concept**: Classifies based on majority vote of K nearest neighbors.

**Real-life Analogy**: "Show me your friends, and I'll tell you who you are."

```python
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

# Sample data: [height, weight] and gender
X = np.array([
    [170, 65], [165, 60], [180, 75], [175, 70],
    [160, 55], [185, 80], [172, 68], [178, 73]
])
y = np.array([0, 0, 1, 1, 0, 1, 0, 1])  # 0=Female, 1=Male

# Train KNN
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X, y)

# Predict for new person
new_person = np.array([[175, 68]])
prediction = knn_model.predict(new_person)
print(f"Predicted gender: {'Male' if prediction[0] == 1 else 'Female'}")

# Visualize
plt.scatter(X[y==0][:, 0], X[y==0][:, 1], color='red', label='Female', s=100)
plt.scatter(X[y==1][:, 0], X[y==1][:, 1], color='blue', label='Male', s=100)
plt.scatter(new_person[:, 0], new_person[:, 1], color='green',
           marker='*', s=300, label='New Person')
plt.xlabel('Height (cm)')
plt.ylabel('Weight (kg)')
plt.legend()
plt.title('KNN Classification')
plt.show()
```

### Perceptron

**Concept**: The simplest neural network - a single neuron.

**Real-life Example**: Binary classification (AND, OR logic gates)

```python
from sklearn.linear_model import Perceptron

# AND gate example
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # AND logic

perceptron = Perceptron(max_iter=1000, random_state=42)
perceptron.fit(X, y)

# Test
for input_data in X:
    prediction = perceptron.predict([input_data])
    print(f"Input: {input_data} -> Output: {prediction[0]}")
```

### Neural Networks (Multilayer Perceptron)

**Concept**: Multiple layers of perceptrons that can learn complex patterns.

**Real-life Analogy**: Like your brain - neurons connected in layers, passing information forward.

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_moons

# Generate non-linear dataset
X, y = make_moons(n_samples=200, noise=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Create Neural Network
nn_model = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)
nn_model.fit(X_train, y_train)

# Evaluate
accuracy = nn_model.score(X_test, y_test)
print(f"Neural Network Accuracy: {accuracy:.4f}")

# Visualize decision boundary
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.scatter(X[y==0][:, 0], X[y==0][:, 1], color='red', label='Class 0')
plt.scatter(X[y==1][:, 0], X[y==1][:, 1], color='blue', label='Class 1')
plt.title('Original Data')
plt.legend()
plt.show()
```

### Support Vector Machine (SVM)

**Concept**: Finds the best boundary (hyperplane) that separates classes with maximum margin.

**Real-life Analogy**: Drawing a line in the sand to separate two groups, keeping the line as far as possible from both groups.

```python
from sklearn.svm import SVC

# Load dataset
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,
                          n_informative=2, random_state=42, n_clusters_per_class=1)

# Train SVM
svm_model = SVC(kernel='linear', C=1.0)
svm_model.fit(X, y)

# Evaluate
accuracy = svm_model.score(X, y)
print(f"SVM Accuracy: {accuracy:.4f}")

# Visualize
plt.scatter(X[y==0][:, 0], X[y==0][:, 1], color='red', label='Class 0')
plt.scatter(X[y==1][:, 0], X[y==1][:, 1], color='blue', label='Class 1')
plt.title('SVM Classification')
plt.legend()
plt.show()
```

### Classification Evaluation Metrics

Understanding how well your classifier performs:

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report

# Sample predictions
y_true = [0, 1, 1, 0, 1, 1, 0, 0, 1, 1]
y_pred = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1]

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)
print("\nBreakdown:")
print(f"True Negatives (TN): {cm[0, 0]}")
print(f"False Positives (FP): {cm[0, 1]}")
print(f"False Negatives (FN): {cm[1, 0]}")
print(f"True Positives (TP): {cm[1, 1]}")

# Metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f"\nAccuracy: {accuracy:.4f} - Overall correctness")
print(f"Precision: {precision:.4f} - Of predicted positives, how many were correct")
print(f"Recall: {recall:.4f} - Of actual positives, how many were caught")
print(f"F1-Score: {f1:.4f} - Harmonic mean of precision and recall")

# Detailed report
print("\nClassification Report:")
print(classification_report(y_true, y_pred))
```

**Real-life interpretation**:

- **Accuracy**: How often is the classifier correct overall?
- **Precision**: When it predicts positive, how often is it right? (Important when false positives are costly - e.g., spam filter)
- **Recall**: Of all actual positives, how many did we catch? (Important when false negatives are costly - e.g., disease detection)
- **F1-Score**: Balance between precision and recall

---

## Unit 5: Clustering

**What is Clustering?** Grouping similar items together without predefined labels.

**Real-life Examples**:

- Customer segmentation
- Document organization
- Image compression
- Anomaly detection

### Distance Metrics

How we measure similarity between data points:

**1. Euclidean Distance**: Straight-line distance (most common)

```python
from scipy.spatial.distance import euclidean, cityblock, cosine

point1 = [1, 2, 3]
point2 = [4, 5, 6]

euclidean_dist = euclidean(point1, point2)
print(f"Euclidean Distance: {euclidean_dist:.4f}")
```

**2. Manhattan Distance**: Sum of absolute differences (like walking in a city grid)

```python
manhattan_dist = cityblock(point1, point2)
print(f"Manhattan Distance: {manhattan_dist:.4f}")
```

**3. Cosine Similarity**: Measures angle between vectors (used for text)

```python
cosine_dist = cosine(point1, point2)
print(f"Cosine Distance: {cosine_dist:.4f}")
```

### K-Means Clustering

**Concept**: Partitions data into K clusters by iteratively assigning points to nearest centroid.

**Real-life Example**: Customer segmentation in retail

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Customer data: [Annual Income, Spending Score]
customers = np.array([
    [15, 39], [16, 81], [17, 6], [18, 77], [19, 40],
    [20, 76], [21, 6], [23, 94], [24, 3], [25, 72],
    [65, 20], [67, 35], [68, 43], [69, 50], [70, 42],
    [71, 10], [72, 48], [73, 85], [74, 20], [75, 30],
    [35, 40], [36, 60], [37, 45], [38, 55], [39, 50],
    [40, 65], [41, 45], [42, 70], [43, 50], [44, 60]
])

# Apply K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(customers)

# Visualize
plt.figure(figsize=(10, 6))
colors = ['red', 'blue', 'green']
for i in range(3):
    cluster_points = customers[clusters == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1],
               c=colors[i], label=f'Cluster {i+1}', s=100, alpha=0.6)

# Plot centroids
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1],
           c='black', marker='X', s=300, label='Centroids')

plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.title('Customer Segmentation using K-Means')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# Interpret clusters
print("Cluster Interpretation:")
for i in range(3):
    cluster_points = customers[clusters == i]
    avg_income = cluster_points[:, 0].mean()
    avg_spending = cluster_points[:, 1].mean()
    print(f"\nCluster {i+1}:")
    print(f"  Average Income: ${avg_income:.2f}k")
    print(f"  Average Spending Score: {avg_spending:.2f}")
```

**Finding Optimal K (Elbow Method)**:

```python
# Elbow Method to find optimal number of clusters
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(customers)
    inertias.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia (Within-cluster sum of squares)')
plt.title('Elbow Method for Optimal K')
plt.grid(True)
plt.show()
```

### Hierarchical Clustering

**Concept**: Creates a tree-like structure (dendrogram) showing how clusters merge.

**Two Approaches**:

1. **Agglomerative (Bottom-up)**: Start with each point as a cluster, merge similar ones
2. **Divisive (Top-down)**: Start with one cluster, split recursively

**Real-life Example**: Organizing a library - books â†’ genres â†’ subgenres

```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Sample data
X = np.array([
    [1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]
])

# Create dendrogram
linkage_matrix = linkage(X, method='ward')

plt.figure(figsize=(10, 6))
dendrogram(linkage_matrix, labels=[f'P{i}' for i in range(len(X))])
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.show()

# Apply Agglomerative Clustering
hierarchical = AgglomerativeClustering(n_clusters=2)
labels = hierarchical.fit_predict(X)

# Visualize clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=200)
plt.title('Hierarchical Clustering Result')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
for i, txt in enumerate([f'P{i}' for i in range(len(X))]):
    plt.annotate(txt, (X[i, 0], X[i, 1]), fontsize=12)
plt.show()

print("Cluster assignments:", labels)
```

---

## Complete End-to-End Machine Learning Project Example

Let's build a complete project: **Predicting whether a person will buy a product**

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Create Dataset
np.random.seed(42)
n_samples = 1000

data = {
    'Age': np.random.randint(18, 70, n_samples),
    'Income': np.random.randint(20000, 150000, n_samples),
    'Time_on_Website': np.random.randint(1, 60, n_samples),
    'Previous_Purchases': np.random.randint(0, 10, n_samples),
}

# Create target variable (purchase or not)
# Higher income, more time, more previous purchases = higher chance of purchase
data['Purchase'] = (
    (data['Income'] > 70000).astype(int) +
    (data['Time_on_Website'] > 30).astype(int) +
    (data['Previous_Purchases'] > 3).astype(int)
) >= 2

df = pd.DataFrame(data)

print("Dataset Info:")
print(df.head())
print(f"\nDataset shape: {df.shape}")
print(f"Purchase distribution:\n{df['Purchase'].value_counts()}")

# Step 2: Data Preprocessing
X = df.drop('Purchase', axis=1)
y = df['Purchase']

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Train Multiple Models
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

results = {}

for name, model in models.items():
    # Train
    model.fit(X_train_scaled, y_train)

    # Predict
    y_pred = model.predict(X_test_scaled)

    # Evaluate
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }

    print(f"\n{'='*50}")
    print(f"{name}")
    print(f"{'='*50}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Cross-validation: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

# Step 4: Visualize Results
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Model Comparison
ax1 = axes[0, 0]
model_names = list(results.keys())
accuracies = [results[name]['accuracy'] for name in model_names]
ax1.bar(model_names, accuracies, color=['blue', 'green', 'orange'])
ax1.set_ylabel('Accuracy')
ax1.set_title('Model Comparison')
ax1.set_ylim([0, 1])
for i, v in enumerate(accuracies):
    ax1.text(i, v + 0.01, f'{v:.4f}', ha='center')

# Confusion Matrix for Best Model
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_predictions = results[best_model_name]['predictions']
cm = confusion_matrix(y_test, best_predictions)
ax2 = axes[0, 1]
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2)
ax2.set_title(f'Confusion Matrix - {best_model_name}')
ax2.set_ylabel('Actual')
ax2.set_xlabel('Predicted')

# Feature Importance (if available)
if hasattr(results[best_model_name]['model'], 'feature_importances_'):
    importances = results[best_model_name]['model'].feature_importances_
    ax3 = axes[1, 0]
    ax3.barh(X.columns, importances)
    ax3.set_xlabel('Importance')
    ax3.set_title('Feature Importance')

# Purchase Distribution
ax4 = axes[1, 1]
df['Purchase'].value_counts().plot(kind='bar', ax=ax4, color=['red', 'green'])
ax4.set_title('Purchase Distribution')
ax4.set_xlabel('Purchase')
ax4.set_ylabel('Count')
ax4.set_xticklabels(['No Purchase', 'Purchase'], rotation=0)

plt.tight_layout()
plt.show()

# Step 5: Make Predictions for New Customers
print("\n" + "="*50)
print("PREDICTIONS FOR NEW CUSTOMERS")
print("="*50)

new_customers = pd.DataFrame({
    'Age': [25, 45, 35],
    'Income': [50000, 120000, 80000],
    'Time_on_Website': [15, 45, 30],
    'Previous_Purchases': [1, 7, 4]
})

new_customers_scaled = scaler.transform(new_customers)
best_model = results[best_model_name]['model']
predictions = best_model.predict(new_customers_scaled)
probabilities = best_model.predict_proba(new_customers_scaled)

for i, row in new_customers.iterrows():
    print(f"\nCustomer {i+1}:")
    print(f"  Age: {row['Age']}, Income: ${row['Income']}, "
          f"Time on Site: {row['Time_on_Website']}min, "
          f"Previous Purchases: {row['Previous_Purchases']}")
    print(f"  Prediction: {'Will Buy' if predictions[i] else 'Won\\'t Buy'}")
    print(f"  Confidence: {probabilities[i][1]:.2%}")
```

---

## Key Takeaways for Beginners

### 1. **Machine Learning Workflow**

1. Understand the problem
2. Collect and prepare data
3. Choose appropriate algorithm
4. Train the model
5. Evaluate performance
6. Make predictions

### 2. **When to Use What**

| Problem Type               | Algorithm to Try First        |
| -------------------------- | ----------------------------- |
| Continuous prediction      | Linear Regression             |
| Binary classification      | Logistic Regression           |
| Multi-class classification | Decision Trees, Random Forest |
| Clustering                 | K-Means                       |
| High-dimensional data      | PCA first                     |
| Text classification        | Naive Bayes                   |
| Complex patterns           | Neural Networks               |

### 3. **Common Pitfalls to Avoid**

- Not scaling features
- Overfitting (model too complex)
- Underfitting (model too simple)
- Not splitting data properly
- Ignoring class imbalance
- Not validating results

### 4. **Best Practices**

- Always split data into train/test sets
- Use cross-validation
- Try multiple models
- Visualize your data
- Start simple, then increase complexity
- Document your work

### 5. **Resources for Learning**

- Practice on Kaggle datasets
- Use scikit-learn documentation
- Join ML communities
- Build projects
- Learn by doing!

---

## Quick Reference Code Templates

### Loading Data

```python
import pandas as pd
df = pd.read_csv('data.csv')
X = df.drop('target', axis=1)
y = df['target']
```

### Train-Test Split

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### Scaling

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### Training and Prediction

```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

### Evaluation

```python
from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(y_test, predictions)
print(classification_report(y_test, predictions))
```

Good luck with your Machine Learning journey! ðŸš€
